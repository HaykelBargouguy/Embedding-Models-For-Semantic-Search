{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\nfrom typing import List, Dict, Any\nimport time\nimport os\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:41:19.601936Z","iopub.execute_input":"2025-04-04T19:41:19.602251Z","iopub.status.idle":"2025-04-04T19:41:19.606653Z","shell.execute_reply.started":"2025-04-04T19:41:19.602222Z","shell.execute_reply":"2025-04-04T19:41:19.605765Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class SemanticSearch:\n    # def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n    # def __init__(self, model_name: str = \"all-MiniLM-L12-v2\"):\n    # def __init__(self, model_name: str = 'sentence-transformers/all-mpnet-base-v2'):\n    # def __init__(self, model_name: str =\"BAAI/bge-base-en-v1.5\"):\n    # def __init__(self, model_name: str = 'intfloat/e5-large-v2'): best\n    # def __init__(self, model_name: str = 'sentence-transformers/all-mpnet-base-v2'):\n    # def __init__(self, model_name: str = 'thenlper/gte-base'):\n    def __init__(self, model_name: str = 'hkunlp/instructor-xl'):\n\n        \"\"\"Initialize the SemanticSearch class with a pre-trained model\"\"\"\n        print(f\"Loading model: {model_name}\")\n        self.model = SentenceTransformer(model_name)\n        self.database_items = []\n        self.database_embeddings = None\n        self.embedding_cache_path = \"embedding_cache.pkl\"\n        \n    def add_documents(self, documents: List[str], use_cache: bool = True) -> None:\n        \"\"\"Add documents to the search database and encode them\"\"\"\n        self.database_items = documents\n        \n        if use_cache and os.path.exists(self.embedding_cache_path):\n            try:\n                with open(self.embedding_cache_path, 'rb') as f:\n                    cached_data = pickle.load(f)\n                    if cached_data.get('documents') == documents:\n                        print(\"Using cached embeddings\")\n                        self.database_embeddings = cached_data.get('embeddings')\n                        return\n            except Exception as e:\n                print(f\"Error loading cache: {e}\")\n        \n        # Encode documents to embeddings\n        print(f\"Encoding {len(documents)} documents...\")\n        start_time = time.time()\n        self.database_embeddings = self.model.encode(documents, show_progress_bar=True)\n        print(f\"Encoding completed in {time.time() - start_time:.2f} seconds\")\n        \n        # Save embeddings to cache\n        if use_cache:\n            with open(self.embedding_cache_path, 'wb') as f:\n                pickle.dump({\n                    'documents': documents,\n                    'embeddings': self.database_embeddings\n                }, f)\n            print(\"Embeddings cached for future use\")\n    \n    def search(self, query: str, top_n: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Perform semantic search for a query against the document database\"\"\"\n        if self.database_embeddings is None or len(self.database_items) == 0:\n            raise ValueError(\"No documents have been added to the search database\")\n        \n        # Encode the query\n        query_embedding = self.model.encode(query)\n        \n        # Calculate cosine similarities\n        similarities = util.pytorch_cos_sim(\n            query_embedding.reshape(1, -1), \n            self.database_embeddings\n        )[0]\n        \n        top_indices = np.argsort(similarities.numpy())[::-1][:top_n]\n        \n        results = []\n        for idx in top_indices:\n            results.append({\n                \"document\": self.database_items[idx],\n                \"score\": float(similarities[idx]),\n                \"position\": len(results) + 1\n            })\n        \n        return results\n    \n    def batch_search(self, queries: List[str], top_n: int = 5) -> Dict[str, List[Dict[str, Any]]]:\n        \"\"\"Perform semantic search for multiple queries at once\"\"\"\n        results = {}\n        for query in queries:\n            results[query] = self.search(query, top_n)\n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:38:42.532987Z","iopub.execute_input":"2025-04-04T18:38:42.533294Z","iopub.status.idle":"2025-04-04T18:38:42.542773Z","shell.execute_reply.started":"2025-04-04T18:38:42.533248Z","shell.execute_reply":"2025-04-04T18:38:42.542090Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"database_items = [\n    \"Hospital A in London UK\",\n    \"Medical Center B in London UK\", \n    \"Clinic C in London UK\",\n    \"London General Hospital\",\n    \"Emergency Care Unit in South London\",\n    \"Royal London Hospital\",\n    \"London Bridge Hospital\",\n    \"Private Healthcare Facility in Central London\",\n    \"St Thomas' Hospital London\",\n    \"Guy's Hospital London UK\",\n    \"University College Hospital London\",\n    \"Healthcare Center in North London\",\n    \"Pediatric Hospital in London\",\n    \"Specialist Cancer Center London UK\",\n    \"Mental Health Facility London\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:38:46.070987Z","iopub.execute_input":"2025-04-04T18:38:46.071310Z","iopub.status.idle":"2025-04-04T18:38:46.075022Z","shell.execute_reply.started":"2025-04-04T18:38:46.071250Z","shell.execute_reply":"2025-04-04T18:38:46.074362Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"search_engine = SemanticSearch()\nsearch_engine.add_documents([\"pediatric medical care london\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:38:47.166532Z","iopub.execute_input":"2025-04-04T18:38:47.166811Z","iopub.status.idle":"2025-04-04T18:39:17.011649Z","shell.execute_reply.started":"2025-04-04T18:38:47.166789Z","shell.execute_reply":"2025-04-04T18:39:17.010798Z"}},"outputs":[{"name":"stdout","text":"Loading model: hkunlp/instructor-xl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9addb85969e4c6bad966f8d75a4ccce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ba134356b6949c588246def84314198"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"438f2e144f2149a0bbc5e5db730da686"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fb1ca740c4641eab91a28ce5185890c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9104427167e4df9a9fdc2024d58cb62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11cffd85277943489310cb140a370495"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6ea9f6f0e2c4acfaaa577f418dd14d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5deda83ac27845869f378df48c0b195d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24773d1f03aa437b84eda0d7e11241b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4161eb9847b142499b6ea02d33bec280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ae7e01be7424f348613a9d91bebc9a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ad194d2bc14f2db5c7475b63894af4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ba16f2189e844bca7b63ba5c2c52fa3"}},"metadata":{}},{"name":"stdout","text":"Encoding 1 documents...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"919c7524091d4b9691ec5e74ab80acc8"}},"metadata":{}},{"name":"stdout","text":"Encoding completed in 2.10 seconds\nEmbeddings cached for future use\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"results = search_engine.search(\"hospitals in london uk\", top_n=5)\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:39:33.297044Z","iopub.execute_input":"2025-04-04T18:39:33.297564Z","iopub.status.idle":"2025-04-04T18:39:33.468027Z","shell.execute_reply.started":"2025-04-04T18:39:33.297512Z","shell.execute_reply":"2025-04-04T18:39:33.467359Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc393bf3371245ddb9b60f76598c3d1f"}},"metadata":{}},{"name":"stdout","text":"[{'document': 'pediatric medical care london', 'score': 0.8226864337921143, 'position': 1}]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:39:34.920167Z","iopub.execute_input":"2025-04-04T18:39:34.920511Z","iopub.status.idle":"2025-04-04T18:39:34.924784Z","shell.execute_reply.started":"2025-04-04T18:39:34.920484Z","shell.execute_reply":"2025-04-04T18:39:34.924100Z"}},"outputs":[{"name":"stdout","text":"[{'document': 'pediatric medical care london', 'score': 0.8226864337921143, 'position': 1}]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def main():\n\n    \n    # Sample queries\n    sample_queries = [\n        \"hospitals in london uk\",\n        \"emergency care in london\",\n        \"pediatric medical care london\",\n        \"private healthcare london\",\n        \"mental health services\"\n    ]\n    \n    search_engine = SemanticSearch()\n    search_engine.add_documents(database_items)\n    \n    # Perform individual search\n    print(\"\\n--- Individual Search ---\")\n    results = search_engine.search(\"hospitals in london uk\", top_n=1)\n    \n    print(f\"Query: hospitals in london uk\")\n    for result in results:\n        print(f\"- {result['document']} (Score: {result['score']:.4f})\")\n    \n    # Perform batch search\n    print(\"\\n--- Batch Search ---\")\n    batch_results = search_engine.batch_search(sample_queries, top_n=3)\n    \n    for query, results in batch_results.items():\n        print(f\"\\nQuery: {query}\")\n        for result in results:\n            print(f\"- {result['document']} (Score: {result['score']:.4f})\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:39:37.068068Z","iopub.execute_input":"2025-04-04T18:39:37.068406Z","iopub.status.idle":"2025-04-04T18:39:41.910334Z","shell.execute_reply.started":"2025-04-04T18:39:37.068380Z","shell.execute_reply":"2025-04-04T18:39:41.909357Z"}},"outputs":[{"name":"stdout","text":"Loading model: hkunlp/instructor-xl\nEncoding 15 documents...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83a5dd2ed6564542a66c86e6d69bfd9b"}},"metadata":{}},{"name":"stdout","text":"Encoding completed in 0.17 seconds\nEmbeddings cached for future use\n\n--- Individual Search ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"193b9023364d45ca93bfabbdcbe8eef8"}},"metadata":{}},{"name":"stdout","text":"Query: hospitals in london uk\n- Hospital A in London UK (Score: 0.9039)\n\n--- Batch Search ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8735af7ea0643419278050456605edb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56658a5144e04ce69c76096b490c1a9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b62341e8b0bd446fba5e5845b11a7458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b09d8400c55e42cd945c1e80b890df5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"666a0f51903e4ff18871d6af404c5e7c"}},"metadata":{}},{"name":"stdout","text":"\nQuery: hospitals in london uk\n- Hospital A in London UK (Score: 0.9039)\n- Pediatric Hospital in London (Score: 0.8796)\n- Royal London Hospital (Score: 0.8769)\n\nQuery: emergency care in london\n- Emergency Care Unit in South London (Score: 0.9090)\n- Hospital A in London UK (Score: 0.8380)\n- Healthcare Center in North London (Score: 0.8133)\n\nQuery: pediatric medical care london\n- Pediatric Hospital in London (Score: 0.9321)\n- Healthcare Center in North London (Score: 0.8202)\n- Medical Center B in London UK (Score: 0.8122)\n\nQuery: private healthcare london\n- Private Healthcare Facility in Central London (Score: 0.9359)\n- Healthcare Center in North London (Score: 0.8246)\n- Hospital A in London UK (Score: 0.8106)\n\nQuery: mental health services\n- Mental Health Facility London (Score: 0.7380)\n- Healthcare Center in North London (Score: 0.6336)\n- Private Healthcare Facility in Central London (Score: 0.6125)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# import torch\n# from sentence_transformers import SentenceTransformer\n# from typing import List\n\n# def process_model(model_name: str, queries: List[str], documents: List[str], top_n: int = 5):\n#     \"\"\"\n#     Load a model, perform semantic search, and release GPU memory.\n#     \"\"\"\n#     print(f\"Processing with model: {model_name}\")\n    \n#     # Load the model\n#     model = SentenceTransformer(model_name)\n    \n#     # Encode documents\n#     print(\"Encoding documents...\")\n#     document_embeddings = model.encode(documents, show_progress_bar=True)\n    \n#     # Process each query\n#     results = []\n#     for query in queries:\n#         print(f\"Processing query: {query}\")\n#         query_embedding = model.encode(query)\n        \n#         # Calculate cosine similarities\n#         similarities = torch.nn.functional.cosine_similarity(\n#             torch.tensor(query_embedding).unsqueeze(0).cuda(),\n#             torch.tensor(document_embeddings).cuda()\n#         )\n        \n#         # Get top N results\n#         top_indices = torch.argsort(similarities, descending=True)[:top_n]\n#         query_results = [\n#             {\"document\": documents[idx], \"score\": float(similarities[idx])}\n#             for idx in top_indices\n#         ]\n#         results.append({\"query\": query, \"results\": query_results})\n    \n#     # Clear GPU memory\n#     del model\n#     torch.cuda.empty_cache()\n    \n#     return results\n\n# def main():\n#     # Define models, queries, and documents\n#     models = [\n#         \"sentence-transformers/all-MiniLM-L6-v2\",\n#         \"sentence-transformers/all-mpnet-base-v2\",\n#         \"hkunlp/instructor-xl\"\n#     ]\n    \n#     queries = [\n#         \"hospitals in london uk\",\n#         \"emergency care in london\",\n#         \"pediatric medical care london\",\n#         \"private healthcare london\",\n#         \"mental health services\"\n#     ]\n    \n#     documents = [\"pediatric medical care london\"]\n    \n#     # Process each model sequentially\n#     for model_name in models:\n#         results = process_model(model_name, queries, documents)\n        \n#         # Print results for each model\n#         print(f\"\\nResults for model: {model_name}\")\n#         for result in results:\n#             print(f\"Query: {result['query']}\")\n#             for res in result['results']:\n#                 print(f\"- {res['document']} (Score: {res['score']:.4f})\")\n\n# if __name__ == \"__main__\":\n#     main()\n\n\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\nfrom typing import List, Dict, Any, Tuple\nimport time\nimport os\nimport pickle\nimport gc\nimport torch\n\nclass SemanticSearch:\n    def __init__(self, model_name: str):\n        \"\"\"Initialize the SemanticSearch class with a pre-trained model\"\"\"\n        print(f\"Loading model: {model_name}\")\n        self.model_name = model_name\n        self.model = SentenceTransformer(model_name)\n        self.database_items = []\n        self.database_embeddings = None\n        self.embedding_cache_path = f\"embedding_cache_{model_name.replace('/', '_')}.pkl\"\n        \n    def add_documents(self, documents: List[str], use_cache: bool = True) -> float:\n        \"\"\"Add documents to the search database and encode them, returns encoding time\"\"\"\n        self.database_items = documents\n        \n        if use_cache and os.path.exists(self.embedding_cache_path):\n            try:\n                with open(self.embedding_cache_path, 'rb') as f:\n                    cached_data = pickle.load(f)\n                    if cached_data.get('documents') == documents:\n                        print(\"Using cached embeddings\")\n                        self.database_embeddings = cached_data.get('embeddings')\n                        return 0.0  # No encoding time when using cache\n            except Exception as e:\n                print(f\"Error loading cache: {e}\")\n        \n        # Encode documents to embeddings\n        print(f\"Encoding {len(documents)} documents...\")\n        start_time = time.time()\n        self.database_embeddings = self.model.encode(documents, show_progress_bar=True)\n        encoding_time = time.time() - start_time\n        print(f\"Encoding completed in {encoding_time:.2f} seconds\")\n        \n        # Save embeddings to cache\n        if use_cache:\n            with open(self.embedding_cache_path, 'wb') as f:\n                pickle.dump({\n                    'documents': documents,\n                    'embeddings': self.database_embeddings\n                }, f)\n            print(\"Embeddings cached for future use\")\n            \n        return encoding_time\n    \n    def search(self, query: str, top_n: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Perform semantic search for a query against the document database\"\"\"\n        if self.database_embeddings is None or len(self.database_items) == 0:\n            raise ValueError(\"No documents have been added to the search database\")\n        \n        # Encode the query\n        query_embedding = self.model.encode(query)\n        \n        # Calculate cosine similarities\n        similarities = util.pytorch_cos_sim(\n            query_embedding.reshape(1, -1), \n            self.database_embeddings\n        )[0]\n        \n        top_indices = np.argsort(similarities.numpy())[::-1][:top_n]\n        \n        results = []\n        for idx in top_indices:\n            results.append({\n                \"document\": self.database_items[idx],\n                \"score\": float(similarities[idx]),\n                \"position\": len(results) + 1\n            })\n        \n        return results\n    \n    def batch_search(self, queries: List[str], top_n: int = 5) -> Tuple[Dict[str, List[Dict[str, Any]]], float]:\n        \"\"\"Perform semantic search for multiple queries at once, returns results and search time\"\"\"\n        results = {}\n        start_time = time.time()\n        for query in queries:\n            results[query] = self.search(query, top_n)\n        search_time = time.time() - start_time\n        return results, search_time\n    \n    def cleanup(self):\n        \"\"\"Clean up model to free memory\"\"\"\n        del self.model\n        if hasattr(self, 'database_embeddings'):\n            del self.database_embeddings\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n        print(f\"Model {self.model_name} has been cleaned up\")\n\n\ndef evaluate_model(model_name, database_items, sample_queries, results_file_path):\n    \"\"\"Evaluate a single model and save results to file\"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"Evaluating model: {model_name}\")\n    print(f\"{'='*50}\")\n    \n    try:\n        # Initialize search engine and measure time\n        start_time = time.time()\n        search_engine = SemanticSearch(model_name)\n        initialization_time = time.time() - start_time\n        \n        # Add documents and get encoding time\n        encoding_time = search_engine.add_documents(database_items)\n        \n        # Perform batch search and get search time\n        batch_results, search_time = search_engine.batch_search(sample_queries, top_n=3)\n        \n        # Calculate average score for each query as a performance metric\n        performance_metrics = {}\n        for query, results in batch_results.items():\n            if results:\n                avg_score = sum(r['score'] for r in results) / len(results)\n                top_score = results[0]['score'] if results else 0\n                performance_metrics[query] = {\n                    'avg_score': avg_score,\n                    'top_score': top_score\n                }\n        \n        avg_top_score = sum(metrics['top_score'] for metrics in performance_metrics.values()) / len(performance_metrics)\n        \n        # Save results to file\n        with open(results_file_path, 'a') as f:\n            f.write(f\"\\n{'='*50}\\n\")\n            f.write(f\"Model: {model_name}\\n\")\n            f.write(f\"{'='*50}\\n\")\n            \n            # Timing metrics\n            f.write(\"TIMING METRICS:\\n\")\n            f.write(f\"Initialization time: {initialization_time:.2f} seconds\\n\")\n            f.write(f\"Encoding time: {encoding_time:.2f} seconds\\n\")\n            f.write(f\"Search time for {len(sample_queries)} queries: {search_time:.2f} seconds\\n\")\n            f.write(f\"Average search time per query: {search_time/len(sample_queries):.4f} seconds\\n\\n\")\n            \n            # Performance metrics\n            f.write(\"PERFORMANCE METRICS:\\n\")\n            f.write(f\"Average top result score: {avg_top_score:.4f}\\n\\n\")\n            \n            # Detailed search results\n            f.write(\"SEARCH RESULTS:\\n\")\n            for query, results in batch_results.items():\n                f.write(f\"\\nQuery: {query}\\n\")\n                f.write(f\"  Average score: {performance_metrics[query]['avg_score']:.4f}\\n\")\n                f.write(f\"  Top score: {performance_metrics[query]['top_score']:.4f}\\n\")\n                for result in results:\n                    f.write(f\"  - {result['document']} (Score: {result['score']:.4f})\\n\")\n            \n            f.write(f\"\\n{'-'*50}\\n\")\n        \n        # Print summary\n        print(f\"Model: {model_name}\")\n        print(f\"Initialization time: {initialization_time:.2f} seconds\")\n        print(f\"Encoding time: {encoding_time:.2f} seconds\")\n        print(f\"Search time: {search_time:.2f} seconds\")\n        print(f\"Average top score: {avg_top_score:.4f}\")\n        \n        # Clean up model\n        search_engine.cleanup()\n        return True\n        \n    except Exception as e:\n        with open(results_file_path, 'a') as f:\n            f.write(f\"\\n{'='*50}\\n\")\n            f.write(f\"Model: {model_name}\\n\")\n            f.write(f\"ERROR: {str(e)}\\n\")\n            f.write(f\"\\n{'-'*50}\\n\")\n        print(f\"Error evaluating model {model_name}: {e}\")\n        return False\n\n\ndef main():\n    # Database items\n    database_items = [\n        \"Hospital A in London UK\",\n        \"Medical Center B in London UK\", \n        \"Clinic C in London UK\",\n        \"London General Hospital\",\n        \"Emergency Care Unit in South London\",\n        \"Royal London Hospital\",\n        \"London Bridge Hospital\",\n        \"Private Healthcare Facility in Central London\",\n        \"St Thomas' Hospital London\",\n        \"Guy's Hospital London UK\",\n        \"University College Hospital London\",\n        \"Healthcare Center in North London\",\n        \"Pediatric Hospital in London\",\n        \"Specialist Cancer Center London UK\",\n        \"Mental Health Facility London\"\n    ]\n    \n    # Sample queries\n    sample_queries = [\n        \"hospitals in london uk\",\n        \"emergency care in london\",\n        \"pediatric medical care london\",\n        \"private healthcare london\",\n        \"mental health services\"\n    ]\n    \n    # List of models to evaluate\n    models = [\n        \"all-MiniLM-L6-v2\",\n        \"all-MiniLM-L12-v2\",\n        \"sentence-transformers/all-mpnet-base-v2\",\n        \"BAAI/bge-base-en-v1.5\",\n        \"intfloat/e5-large-v2\",\n        \"thenlper/gte-base\",\n        \"hkunlp/instructor-xl\"\n    ]\n    \n    # Create results directory if it doesn't exist\n    os.makedirs(\"/kaggle/working\", exist_ok=True)\n    \n    # Create results file\n    results_file_path = \"/kaggle/working/model_evaluation_results.txt\"\n    with open(results_file_path, 'w') as f:\n        f.write(\"SEMANTIC SEARCH MODEL EVALUATION RESULTS\\n\")\n        f.write(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        f.write(f\"Number of documents: {len(database_items)}\\n\")\n        f.write(f\"Number of queries: {len(sample_queries)}\\n\\n\")\n        \n        f.write(\"DATABASE ITEMS:\\n\")\n        for i, item in enumerate(database_items):\n            f.write(f\"{i+1}. {item}\\n\")\n        f.write(\"\\n\")\n        \n        f.write(\"SAMPLE QUERIES:\\n\")\n        for i, query in enumerate(sample_queries):\n            f.write(f\"{i+1}. {query}\\n\")\n        f.write(\"\\n\")\n    \n    # Evaluate each model\n    successful_models = 0\n    for model_name in models:\n        if evaluate_model(model_name, database_items, sample_queries, results_file_path):\n            successful_models += 1\n            \n    # Write summary\n    with open(results_file_path, 'a') as f:\n        f.write(f\"\\n{'='*50}\\n\")\n        f.write(f\"SUMMARY\\n\")\n        f.write(f\"{'='*50}\\n\")\n        f.write(f\"Total models evaluated: {len(models)}\\n\")\n        f.write(f\"Successful evaluations: {successful_models}\\n\")\n        f.write(f\"Failed evaluations: {len(models) - successful_models}\\n\")\n        \n    print(f\"\\nEvaluation complete. Results saved to {results_file_path}\")\n    print(f\"Successful models: {successful_models}/{len(models)}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T20:08:14.254288Z","iopub.execute_input":"2025-04-04T20:08:14.254660Z","iopub.status.idle":"2025-04-04T20:09:17.398279Z","shell.execute_reply.started":"2025-04-04T20:08:14.254630Z","shell.execute_reply":"2025-04-04T20:09:17.397501Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nEvaluating model: all-MiniLM-L6-v2\n==================================================\nLoading model: all-MiniLM-L6-v2\nEncoding 15 documents...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97af00d32e454db68c10449050364497"}},"metadata":{}},{"name":"stdout","text":"Encoding completed in 0.06 seconds\nEmbeddings cached for future use\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0a136687da4450bc9646a9414f9305"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15401a74e3ed4983aa68d9cba6a87c82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"675d533ea9f841cea2a93642200f8f51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0dcc6a4777249118a6b29b8ab043571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7900c2102434201b9ab0749ef5fe600"}},"metadata":{}},{"name":"stdout","text":"Model: all-MiniLM-L6-v2\nInitialization time: 1.02 seconds\nEncoding time: 0.06 seconds\nSearch time: 0.19 seconds\nAverage top score: 0.8516\nModel all-MiniLM-L6-v2 has been cleaned up\n\n==================================================\nEvaluating model: all-MiniLM-L12-v2\n==================================================\nLoading model: all-MiniLM-L12-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"622324d2ad6d40a1b17d988f837d7e2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"706b1b9695874b92b1b601f9ed5aa677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e4374b807b14eabba99caff73faa4a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac589c1de8d4cc7a9b3b6ca1a2f1d11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"033055a118694798885368a3ca1dd36d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f7fd28454ae481aa76cd37c7f2fe494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6f908a0986d4a2887e84858c2d24649"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2794ab5fe0d44a60b8cb8dd0da8ff180"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f58a03e2e7204ca29126bdf683255ab5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22de2d73cd0243f6afb29aac890de760"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63eac23b23f542e9afc5257f6a147857"}},"metadata":{}},{"name":"stdout","text":"Encoding 15 documents...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1827bcf99ab47d2acedaf98aa0c7dd9"}},"metadata":{}},{"name":"stdout","text":"Encoding completed in 0.03 seconds\nEmbeddings cached for future use\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0983faa257eb46d4905b2ee389ed3fba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b74a59cfba4dc985be3cd357102083"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6104ce7054444cb58d13e3c427c1bcd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ece3e154b9f043b4b6dc4a3db8015b8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ec79d7bfd6146f2b4eea164969750f8"}},"metadata":{}},{"name":"stdout","text":"Model: all-MiniLM-L12-v2\nInitialization time: 5.81 seconds\nEncoding time: 0.03 seconds\nSearch time: 0.12 seconds\nAverage top score: 0.8439\nModel all-MiniLM-L12-v2 has been cleaned up\n\n==================================================\nEvaluating model: sentence-transformers/all-mpnet-base-v2\n==================================================\nLoading model: sentence-transformers/all-mpnet-base-v2\nEncoding 15 documents...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd415a96aa9644dcb916cdae694c70c7"}},"metadata":{}},{"name":"stdout","text":"Encoding completed in 0.03 seconds\nEmbeddings cached for future use\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e920af848be54f85acaca1907105a7f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1d195b4f2044010a510672a3a2e66bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"664bc275c53c4dc3910b3a310133b8b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f99ab31ab0d74c38a098e8f3dc0d4ee3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"467a57e67e15445ca0902e75e5b46991"}},"metadata":{}},{"name":"stdout","text":"Model: sentence-transformers/all-mpnet-base-v2\nInitialization time: 1.15 seconds\nEncoding time: 0.03 seconds\nSearch time: 0.13 seconds\nAverage top score: 0.8380\nModel sentence-transformers/all-mpnet-base-v2 has been cleaned up\n\n==================================================\nEvaluating model: BAAI/bge-base-en-v1.5\n==================================================\nLoading model: BAAI/bge-base-en-v1.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4eef02f15de458daa6eb384da8ea246"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a986759576174248aa18c0552de6cb60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288957f4b111412d80b5f24b8225e173"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"367712a3d88b45c5b9dd9ab8b60f59a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a6a7d8f616e4c64b2baf83f15f47aac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7203b9645e944b9c95418bb76ce75695"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"647eb2d89cfb43d6b3ff9432db0c294c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09653e25f93943269c42438f84c11d7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcb0c37160024c9988de3f94f9a1827c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8b5bc50ca6548588fe3770e826bfb46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"604e4b83acc44656b75a7e6cb255282c"}},"metadata":{}},{"name":"stdout","text":"Encoding 15 documents...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffa4b249c2ec4180915639f6e4ade917"}},"metadata":{}},{"name":"stdout","text":"Encoding completed in 0.03 seconds\nEmbeddings cached for future use\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32c1eaa206d3417dbfe997a11eaa12ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b332415bf61469282872e90fc1600a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8b688d3cbb4450c97499b9c1be903bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc7d5d2d10314afd8757b9aad935d27e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b717f603c6e646a5bc564cfe8089beee"}},"metadata":{}},{"name":"stdout","text":"Model: BAAI/bge-base-en-v1.5\nInitialization time: 17.13 seconds\nEncoding time: 0.03 seconds\nSearch time: 0.14 seconds\nAverage top score: 0.8659\nModel BAAI/bge-base-en-v1.5 has been cleaned up\n\n==================================================\nEvaluating model: intfloat/e5-large-v2\n==================================================\nLoading model: intfloat/e5-large-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a87d040e39646e6a6073b3f07a25202"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/67.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbc12bba38074d3e8653a0f0c182a10b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92a740d28ba240adbfced8eb14724752"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85293cac492e4f02a86fca689c8804a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a615b81f8b340bf8533cd49f28be2a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ee65424fdf948eaa6aae6d0b97f809f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd1585db3b8640b985b44e1043bb0631"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f4c81f302342cb88717cb8ef98040e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25c32409f79b477bba4d155536f31ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a6e141d72294c66b52c57013c8241ac"}},"metadata":{}},{"name":"stdout","text":"Encoding 15 documents...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0143930d8bbe43758cf0d8c39d222037"}},"metadata":{}},{"name":"stdout","text":"Encoding completed in 0.06 seconds\nEmbeddings cached for future use\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cec94637ffc4070a6c95a8e5fcec165"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a13863180384ca3891f63005e36f8b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c53897669604316afa38e2d97ce3641"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caebdbf7f5904a16955d0d55c97f4adb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ced8ad23edb0469aad2b7be7ff93a915"}},"metadata":{}},{"name":"stdout","text":"Model: intfloat/e5-large-v2\nInitialization time: 25.35 seconds\nEncoding time: 0.06 seconds\nSearch time: 0.17 seconds\nAverage top score: 0.9178\nModel intfloat/e5-large-v2 has been cleaned up\n\n==================================================\nEvaluating model: thenlper/gte-base\n==================================================\nLoading model: thenlper/gte-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032ec8b0c3df4f94966b8015a74181c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"522ab15baa4d45779cb4231766923f87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e64ba56ffeba4a86b6f880df9c6c8773"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/618 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bc9d2c969db44628cb3aad599acb268"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/219M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49c0468e2f1942e0ba8ee3152cf30e1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab63ab7a81ba4c5a84ce1c40ab51cb8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb054e10763a4806bc0e607e7462be40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebd3cee251654ff9aec87549e886d039"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4864a3bd6124ec9907d8a5c7926e714"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6b623bab6ae480c88b07627c689922e"}},"metadata":{}},{"name":"stdout","text":"Encoding 15 documents...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52e9b29906884f5d92a47d54071c4272"}},"metadata":{}},{"name":"stdout","text":"Encoding completed in 0.03 seconds\nEmbeddings cached for future use\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e57f38a9fba949efa1a6e8220f9de237"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba73574b5c0d4eceac156a10fcd7c886"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45d708aedaca43919e1861975e6c6fe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3085dbf6ee694a24b26ef8d37c740d5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25cc9cce525a42a4acde1ef929373ee2"}},"metadata":{}},{"name":"stdout","text":"Model: thenlper/gte-base\nInitialization time: 4.27 seconds\nEncoding time: 0.03 seconds\nSearch time: 0.13 seconds\nAverage top score: 0.9328\nModel thenlper/gte-base has been cleaned up\n\n==================================================\nEvaluating model: hkunlp/instructor-xl\n==================================================\nLoading model: hkunlp/instructor-xl\nEncoding 15 documents...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de8f21af823840c884bfa2db3e658d83"}},"metadata":{}},{"name":"stdout","text":"Encoding completed in 0.16 seconds\nEmbeddings cached for future use\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f11468242fd54799bd472b35699fd5bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea2485735b1490b8639ffb532ed425f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d19ef2accfce47a7b102a818ad307bab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2332f0e38b54422ebc581bbed12b8a87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb81c32de4b84842aacd00220a33d076"}},"metadata":{}},{"name":"stdout","text":"Model: hkunlp/instructor-xl\nInitialization time: 4.16 seconds\nEncoding time: 0.16 seconds\nSearch time: 0.28 seconds\nAverage top score: 0.8838\nModel hkunlp/instructor-xl has been cleaned up\n\nEvaluation complete. Results saved to /kaggle/working/model_evaluation_results.txt\nSuccessful models: 7/7\n","output_type":"stream"}],"execution_count":3}]}